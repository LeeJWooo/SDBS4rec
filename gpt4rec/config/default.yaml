dataset:
  max_seq_len: 15
  min_user_interactions: 5
  splits: [0.8, 0.1, 0.1]

prompt:
  preamble: |
    Previously, the customer has bought:
  postamble: |
    In the future, the customer wants to buy

training:
  model_name: gpt2
  epochs: 20
  lr: 1.0e-4
  weight_decay: 0.01
  warmup_steps: 2000
  per_device_train_batch_size: 8
  per_device_eval_batch_size: 8
  gradient_accumulation_steps: 2
  fp16: true

inference:
  beam_size: 20
  num_queries: 20
  max_gen_tokens: 24
  temperature: 0.9
  top_p: 0.95
  stop_tokens: ["
"]

retrieval:
  K: 40
  k1: 1.2   # BM25
  b: 0.75

evaluation:
  K_list: [5, 10, 20, 40]
